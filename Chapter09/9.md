### Spark Installation
```
# Assuming on a Centos 7 instance
# Java Installation. Refer OpenJDK Docs for further information[2]
yum install java-1.8.0-openjdk

# Spark Installation
curl -# https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop2.7.tgz | tar zx -C /usr/lib

# Setting Spark environment
cat > /etc/profile.d/spark.sh << EOF
SPARK_VERSION=2.3.1
SPARK_HOME=/usr/lib/spark-$SPARK_VERSION-bin-hadoop2.7
PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
EOF

# Sourcing profile for setting env variables
source /etc/profile

# Generating log4j.properties from log4j.properties.template to reduce chattiness
sed "s/log4j.rootCategory=.*/log4j.rootCategory=ERROR, console/g" $SPARK_HOME/conf/log4j.properties.template > $SPARK_HOME/conf/log4j.properties
```

### Spark from All in one docker
```
# Pull from dockerhub
docker pull malepati/cassandra-spark-jupyter:latest

# Build from source
# Clone git repo https://github.com/malepati/book
git clone git@github.com:malepati/book.git
cd book/MasteringApacheCassandra3rdEdition/docker
docker build -t malepati/cassandra-spark-jupyter:latest cassandra-spark-jupyter/.

# Running docker container locally based on SPARK_CLI(pyspark/jupyter/sparkR)
docker run \
--name demo \
-p=4040:4040 \
-p=4041:4041 \
-p=7077:7077 \
-p=8080:8080 \
-p=8081:8081 \
-p=8082:8082 \
-p=9042:9042 \
-e 'CS_HOST=127.0.0.1' \
-e 'CS_DC=dc1' \
-e 'CS_UNAME=cassandra' \
-e 'CS_PWD=cassandra' \
--rm -it malepati/cassandra-spark-jupyter:latest
```

### Spark Configuration
```
spark.master spark://127.0.0.1:7077
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.driver.memory 5g
spark.executor.extraJavaOptions -XX:+PrintGCDetails
```

### Spark Master, Worker and PYSpark Application
```
$SPARK_HOME/sbin/start-master.sh -h 127.0.0.1
$SPARK_HOME/sbin/start-master.sh -h 127.0.0.1 spark://127.0.0.1:7077
$SPARK_HOME/bin/pyspark \
--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 \
--master spark://127.0.0.1:7077 \
--conf spark.driver.memory=<1/4th of memory in GB>g \
--conf spark.executor.memory=<1/2th of memory in GB>g \
--conf spark.driver.maxResultSize=<1/4th of memory in GB>g \
--conf spark.cassandra.connection.host=<cassanra contact point> \
--conf spark.cassandra.connection.local_dc=<dc name> \
--conf spark.cassandra.auth.username=<cassandra username> \
--conf spark.cassandra.auth.password=<cassandra password> \
--conf spark.cassandra.input.consistency.level=<consistency level> \
--conf spark.cassandra.connection.ssl.enabled=true \
--conf spark.cassandra.connection.ssl.trustStore.path=<absolutepath>/truststore \
--conf spark.cassandra.connection.ssl.trustStore.password=<truststore password>
```

### PYSpark Data access from cassandra
```
_keyspace = 'demo'
offers = sqlContext.read.format('org.apache.spark.sql.cassandra').load(table='offers', keyspace=_keyspace)
orders = sqlContext.read.format('org.apache.spark.sql.cassandra').load(table='orders', keyspace=_keyspace)

temp = orders.join(offers, orders.itemid == offers.itemid)
result = temp.select(orders.userid, offers.offerid)

result.show(20, False)

result.distinct().sort('userid').show(20, False)
```

### R Installation
```
yum install epel-release
yum install openssl-devel
yum install libxml2-devel
yum install curl-devel
yum install R
```

### SparkR
```
$SPARK_HOME/bin/SparkR \
--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 \
--master local[*] \
--conf spark.driver.memory=<1/4th of memory in GB>g \
--conf spark.executor.memory=<1/2th of memory in GB>g \
--conf spark.driver.maxResultSize=<1/4th of memory in GB>g \
--conf spark.cassandra.connection.host=<cassanra contact point> \
--conf spark.cassandra.connection.local_dc=<dc name> \
--conf spark.cassandra.auth.username=<cassandra username> \
--conf spark.cassandra.auth.password=<cassandra password> \
--conf spark.cassandra.input.consistency.level=<consistency level> \
--conf spark.cassandra.connection.ssl.enabled=true \
--conf spark.cassandra.connection.ssl.trustStore.path=<absolutepath>/truststore \
--conf spark.cassandra.connection.ssl.trustStore.password=<truststore password>

library(sparklyr)

config <- spark_config()
config$sparklyr.defaultPackages = "com.datastax.spark:spark-cassandra-connector_2.11:2.3.0"
config$spark.driver.host = '127.0.0.1'
config$spark.cassandra.connection.host = '127.0.0.1'
config$spark.cassandra.auth.username = 'cassandra'
config$spark.cassandra.auth.password = 'cassandra'
config$spark.cassandra.connection.local_dc = 'dc1'

sc <- spark_connect(
master = 'spark://127.0.0.1:7077',
spark_home = '/usr/lib/spark-2.3.1-bin-hadoop2.7',
config = config
)

offers_df <- sparklyr:::spark_data_read_generic(
  sc, "org.apache.spark.sql.cassandra", "format",
  list(keyspace = "demo", table = "offers")
  ) %>%
  invoke("load")

offers_tbl <- sparklyr:::spark_partition_register_df(
         sc, offers_df, name = "offers", repartition = 0, memory = TRUE)

offers_tbl

library(dplyr)

orders_df <- sparklyr:::spark_data_read_generic(
  sc, "org.apache.spark.sql.cassandra", "format",
  list(keyspace = "demo", table = "orders")
  ) %>%
  invoke("load")

orders_tbl <- sparklyr:::spark_partition_register_df(
         sc, orders_df, name = "orders", repartition = 0, memory = TRUE)

filter(orders_tbl, itemid == 'item6')
```

### RStudio
```
rpm -ivh https://download1.rstudio.org/rstudio-1.1.456-x86_64.rpm
# Once RStudio is opened, R shell would be on left bottom where you can run below commands.
# Remove authentication or ssl parameters based on cassandra side enabling
install.packages(c("sparklyr","dplyr"))
library(sparklyr)
library(dplyr)
spark_install("2.1.1")

config <- spark_config()
config$sparklyr.defaultPackages = "com.datastax.spark:spark-cassandra-connector_2.11:2.0.1"
config$spark.cassandra.connection.host = '127.0.0.1'
config$spark.cassandra.auth.username = 'cassandra'
config$spark.cassandra.auth.password = 'cassandra'
config$spark.cassandra.connection.local_dc = 'dc1'

sc <- spark_connect(
master = 'local',
spark_home = spark_home_dir(),
config = config
)

# Import to locally
cass_df <- sparklyr:::spark_data_read_generic(
  sc, "org.apache.spark.sql.cassandra", "format",
  list(keyspace = "system_auth", table = "roles")
  ) %>%
  invoke("load")

# Register as a table
cass_tbl <- sparklyr:::spark_partition_register_df(
         sc, cass_df, name = "roles", repartition = 0, memory = TRUE)

View(cass_tbl)         
```

### Jupyter
```
pip install jupyter
# To generate default config file
jupyter notebook --generate-config

# To set initial password instead of token
jupyter notebook password

# With out both password and token for testing purpose add --NotebookApp.token=''
jupyter notebook  --NotebookApp.iopub_data_rate_limit=1.0e10 --no-browser --port 8082 --allow-root --ip=0.0.0.0 --NotebookApp.token=''

mkdir /usr/lib/jupyter
jupyter notebook  --NotebookApp.iopub_data_rate_limit=1.0e10 --no-browser --port 8082 --allow-root --ip=0.0.0.0 --NotebookApp.token='' --notebook-dir=/usr/lib/jupyter
```

### PYSpark on Jupyter
```
pip install findspark
pip install pyspark

import os
import sys
import findspark

findspark.init()
from pyspark import SparkContext, SparkConf
from pyspark.sql.functions import *
from pyspark.sql import *

os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 --master spark://127.0.0.1:7077 pyspark-shell'
conf = SparkConf()
conf.set("spark.cassandra.auth.username", "cassandra")
conf.set("spark.cassandra.auth.password", "cassandra")
conf.set("spark.cassandra.connection.host", "127.0.0.1")
conf.set("spark.cassandra.connection.local_dc", "dc1")

conf.setAppName('demo')
sc = SparkContext(conf=conf)
sqlContext = SQLContext(sc)

_keyspace = 'demo'
orders = sqlContext.read.format('org.apache.spark.sql.cassandra').load(table='orders', keyspace=_keyspace)
result = orders.groupby(orders.userid).count()
result.sort('userid').show(20, False)
```

###
```
```

###
```
```

###
```
```
